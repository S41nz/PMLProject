---
title: "An AI Model Towards a Smarter Fitness"
author: "Pablo Sainz"
date: "`r Sys.Date()`"
output:
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Abstract

Having a healthy lifestyle is not only about how much time is spent in fitness-related activities but also how well or efective is spent. What this paper will propose and address is to build a model based on Machine Learning techniques to be able to effectively classify and predict physical activity by a given subject and being able to determine which type of movement is being done.

## Target Data

```{r, echo=FALSE,warning=FALSE,message=FALSE}
library(caret)
library(dplyr)

# Load the raw training data set
rawTrainingSet <- read.csv("C:\\Users\\psainza\\Documents\\Course8Project1\\pml-training.csv")
# Load the raw testing data set
rawTestSet <- read.csv("C:\\Users\\psainza\\Documents\\Course8Project1\\pml-testing.csv")
#Save the classe column for data frame building purposes
classeColumn <- rawTrainingSet$classe
```

The data sets on which the present work is based consist in the following:

- One training set of `r length(names(rawTrainingSet))` variables with `r nrow(rawTrainingSet)` observations.
- One test set of `r length(names(rawTestSet))` variables with `r nrow(rawTestSet)` observations.
- Both data sets contains observations mapped to `r length(unique(rawTrainingSet$classe))` classes.

## Data Preparation

While the initial presentation of the training set might look considerable, after an initial preprocessing it can be found that only `r sum(complete.cases(rawTrainingSet))` observations are complete. By completeness it is meant having values present on all the variables available.

Considering the above mentioned, then we proceed to filter out all the variables that contain empty or NA values and non useful columns for classification purposes as illustrated in the following code:

```{r,echo=TRUE}
# First we need to force the numeric format in the data frames to obtain the NA
numericTrainingSet <-rawTrainingSet[,sapply(rawTrainingSet, is.numeric)]
numericTestSet <-rawTestSet[,sapply(rawTestSet, is.numeric)]

# Once that we have all the non-existing values properly set to NA then we proceed to remove the 
# variables with NA values present
nonNATrainingSet <- numericTrainingSet[,colSums(is.na(numericTrainingSet)) == 0]
nonNATestSet <- numericTestSet[,colSums(is.na(numericTestSet)) == 0]
#Then add the classe column to the training set
nonNATrainingSet$classe = classeColumn

nonUsefulColsRegEx <- "X|timestamp"
usefulTrainingColumns <- !grepl(nonUsefulColsRegEx, colnames(nonNATrainingSet))
usefulTrainingColNames <- colnames(nonNATrainingSet)[usefulTrainingColumns]
targetTrainingSet = nonNATrainingSet %>% 
  select(usefulTrainingColNames)

usefulTestColumns <- !grepl(nonUsefulColsRegEx, colnames(nonNATestSet))
usefulTestColNames <- colnames(nonNATestSet)[usefulTestColumns]
targetTestSet = nonNATestSet %>% select(usefulTestColNames)
```

After that filtering now we have the following sets:

- One training set of `r length(names(targetTrainingSet))` variables with `r nrow(targetTrainingSet)` observations.
- One test set of `r length(names(targetTestSet))` variables with `r nrow(targetTestSet)` observations.

Once that our initial filtering is prepared, then we proceed to create our validation data set. For the purposes of the present work, we will proceed with a split criterion of 80%-20% to enable our model validation, as illustrated in the following code:
```{r echo=TRUE}
#Set a seed to enable a reproducible data split
inTrainPivot <- createDataPartition(targetTrainingSet$classe, p=0.80, list=F)
finalTrainingSet <- targetTrainingSet[inTrainPivot, ]
validationSet <- targetTrainingSet[-inTrainPivot, ]

```

## Proposed Model: Random Forest

Considering the present problem at hand, which is a classification problem, it has been proposed a classification model based on random forest techniques in order to provide a prediction model based on the provided data.

The proposed model is illustrated with the following code:
```{r,echo=TRUE}

# First establish a training control to make the training part less resource consuming
trainingControl <- trainControl(method = "cv",7,allowParallel = TRUE)
#Create the model
rfModel <- train(classe ~ ., data = finalTrainingSet,method = "rf", trControl=trainingControl,ntree=200)
```
## Model Results

Based on the model proposed on the previous section, we obtain a top accuracy of `r rfModel$results$Accuracy[2]` per the comparison illustrated by he following figure:
```{r,echo=FALSE,fig.cap='Model Training performance'}
plot(rfModel$results$mtry,rfModel$results$Accuracy,xlab = "Number of variables per split",ylab = "Model accuracy",main = "Model training performance")
```

Based on that, we proceed to predict on the created validation set using the generated model, obtaining the following performance as illustrated by the following confusion matrix:

```{r,echo=TRUE}

# Now get the predictor based on the trained model
rfPredictor <- predict(rfModel,validationSet)
#Then create the corresponding confussion matrix
confusionMatrix(validationSet$classe,rfPredictor)
```

Finally we exercise the prediction against the test data targeted as part of the present work, which gives the following results:

```{r,echo=TRUE}

# Now get the predictor based on the trained model
rfTestPredictor <- predict(rfModel,targetTestSet[,-length((names(targetTestSet)))])
rfTestPredictor
```

## Conclusions

The results illustrated in the previous section confirms the initial assumption on which for classification problems, the random forest method can be used safely in terms of saving time in terms of feature selection for model training. In this particular case and given the nature of the random forest technique we can safely use as training set by discarding only the incomplete variables. By following that approach we obtained accuracy levels up to `r rfModel$results$Accuracy[2]` which can be considered as acceptable for the purposes of the present work.

